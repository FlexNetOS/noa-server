model_name,model_family,parameter_count,quantization,queen_fitness_score,reasoning_score,json_reliability,inference_speed_cpu,inference_speed_gpu,memory_gb,vram_gb,context_size,file_size_gb,license,download_url,tier,use_cases,recommended_for_queen,notes
Phi-3.5-mini-instruct,Phi,3.8B,Q4_K_M,0.95,0.95,0.98,40,120,2.5,2.5,4096,2.3,MIT,https://huggingface.co/microsoft/Phi-3.5-mini-instruct-gguf,Premium,"strategic-planning,coordination,reasoning,json-output",Yes,Perfect benchmark scores; Best default choice for Queen; MIT license
Phi-3.5-mini-instruct,Phi,3.8B,Q5_K_M,0.96,0.96,0.99,35,110,3.0,3.0,4096,2.8,MIT,https://huggingface.co/microsoft/Phi-3.5-mini-instruct-gguf,Premium,"strategic-planning,coordination,reasoning,json-output",Yes,Higher quality than Q4; Slightly slower but better accuracy
Phi-3.5-mini-instruct,Phi,3.8B,Q8_0,0.97,0.97,0.99,28,95,4.2,4.2,4096,3.9,MIT,https://huggingface.co/microsoft/Phi-3.5-mini-instruct-gguf,Premium,"strategic-planning,coordination,reasoning,critical-tasks",Yes,Maximum quality; Best for critical coordination decisions
Llama-3.1-8B-Instruct,Llama,8B,Q4_K_M,0.92,0.94,0.95,25,90,5.0,5.0,8192,4.8,Llama-3-Community,https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct-gguf,Premium,"strategic-planning,reasoning,complex-coordination,long-context",Yes,Excellent reasoning; Large context window; Needs more VRAM
Llama-3.1-8B-Instruct,Llama,8B,Q5_K_M,0.93,0.95,0.96,22,85,6.0,6.0,8192,5.8,Llama-3-Community,https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct-gguf,Premium,"strategic-planning,reasoning,complex-coordination,long-context",Yes,Higher quality; Best reasoning for complex tasks
Qwen2-7B-Instruct,Qwen,7B,Q4_K_M,0.88,0.90,0.92,30,95,4.5,4.5,32768,4.3,Apache-2.0,https://huggingface.co/Qwen/Qwen2-7B-Instruct-gguf,Balanced,"coordination,reasoning,multi-lingual,long-context",Yes,Very large context window; Good multilingual support
Qwen2-7B-Instruct,Qwen,7B,Q5_K_M,0.89,0.91,0.93,27,88,5.2,5.2,32768,5.1,Apache-2.0,https://huggingface.co/Qwen/Qwen2-7B-Instruct-gguf,Balanced,"coordination,reasoning,multi-lingual,long-context",Yes,Better quality than Q4; Excellent for complex coordination
Gemma2-9B-Instruct,Gemma,9B,Q4_K_M,0.90,0.93,0.90,20,80,5.5,5.5,8192,5.2,Gemma-License,https://huggingface.co/google/gemma-2-9b-it-gguf,Premium,"strategic-planning,reasoning,safety-focused",Yes,Google model; Strong reasoning; Safety-focused outputs
Gemma2-2B-Instruct,Gemma,2B,Q4_K_M,0.85,0.85,0.88,50,140,1.8,1.8,8192,1.6,Gemma-License,https://huggingface.co/google/gemma-2-2b-it-gguf,Balanced,"coordination,fast-inference,resource-efficient",Yes,Fastest option with good quality; Great for CPU-only setups
Gemma2-2B-Instruct,Gemma,2B,Q5_K_M,0.86,0.86,0.89,45,130,2.1,2.1,8192,1.9,Gemma-License,https://huggingface.co/google/gemma-2-2b-it-gguf,Balanced,"coordination,fast-inference,resource-efficient",Yes,Better quality than Q4; Still very fast
Qwen2-1.5B-Instruct,Qwen,1.5B,Q4_K_M,0.78,0.80,0.85,60,160,1.2,1.2,32768,0.98,Apache-2.0,https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-gguf,Lightweight,"basic-coordination,fast-inference,minimal-resources",Yes,Smallest viable model for Queen; Very large context; Minimal RAM
Qwen2-1.5B-Instruct,Qwen,1.5B,Q5_K_M,0.80,0.82,0.87,55,150,1.4,1.4,32768,1.15,Apache-2.0,https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-gguf,Lightweight,"basic-coordination,fast-inference,minimal-resources",Yes,Better quality in small package
Mistral-7B-Instruct-v0.3,Mistral,7B,Q4_K_M,0.87,0.88,0.90,28,92,4.5,4.5,8192,4.4,Apache-2.0,https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3-gguf,Balanced,"coordination,reasoning,general-purpose",Yes,Popular model; Good all-around performance; Apache license
Mistral-7B-Instruct-v0.3,Mistral,7B,Q5_K_M,0.88,0.89,0.91,25,85,5.3,5.3,8192,5.2,Apache-2.0,https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3-gguf,Balanced,"coordination,reasoning,general-purpose",Yes,Higher quality Mistral variant
DeepSeek-Coder-6.7B-Instruct,DeepSeek,6.7B,Q4_K_M,0.84,0.86,0.94,32,100,4.2,4.2,4096,4.0,DeepSeek-License,https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-instruct-gguf,Balanced,"code-coordination,technical-tasks,json-output",Yes,Excellent for code-related coordination; High JSON reliability
Phi-3-mini-4k-instruct,Phi,3.8B,Q4_K_M,0.93,0.93,0.96,38,115,2.5,2.5,4096,2.3,MIT,https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf,Premium,"coordination,reasoning,json-output",Yes,Previous gen Phi; Still excellent; MIT license
OpenHermes-2.5-Mistral-7B,Mistral,7B,Q4_K_M,0.86,0.88,0.93,28,90,4.5,4.5,8192,4.4,Apache-2.0,https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B-gguf,Balanced,"coordination,instruction-following,json-output",Yes,Fine-tuned for instruction following; High JSON reliability
Nous-Hermes-2-Mixtral-8x7B-DPO,Mixtral,47B,Q4_K_M,0.94,0.96,0.94,8,60,28.0,28.0,32768,26.5,Apache-2.0,https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO-gguf,Premium,"complex-strategic-planning,advanced-reasoning,large-scale-coordination",No,Highest quality reasoning; Requires high-end GPU; For advanced deployments only
Qwen2-0.5B-Instruct,Qwen,0.5B,Q4_K_M,0.70,0.72,0.80,80,180,0.6,0.6,32768,0.35,Apache-2.0,https://huggingface.co/Qwen/Qwen2-0.5B-Instruct-gguf,Lightweight,"minimal-coordination,emergency-fallback",No,Emergency fallback only; Very fast but limited reasoning
Llama-3.2-1B-Instruct,Llama,1B,Q4_K_M,0.75,0.78,0.82,65,170,0.8,0.8,8192,0.68,Llama-3-Community,https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct-gguf,Lightweight,"basic-coordination,mobile-deployment",Yes,Newest small Llama; Good for edge deployment
