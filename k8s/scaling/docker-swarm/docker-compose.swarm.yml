version: '3.9'

# Docker Swarm Stack Configuration for Noa Server
# Deploy with: docker stack deploy -c docker-compose.swarm.yml noa

# Network configuration
networks:
  frontend:
    driver: overlay
    attachable: true
    ipam:
      config:
        - subnet: 10.0.1.0/24

  backend:
    driver: overlay
    attachable: true
    ipam:
      config:
        - subnet: 10.0.2.0/24

  database:
    driver: overlay
    attachable: true
    internal: true
    ipam:
      config:
        - subnet: 10.0.3.0/24

# Volume configuration
volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  rabbitmq_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local

# Secrets
secrets:
  postgres_password:
    external: true
  jwt_secret:
    external: true
  api_key:
    external: true

# Configs
configs:
  nginx_config:
    external: true
  prometheus_config:
    external: true

services:
  # Load Balancer (NGINX)
  nginx:
    image: nginx:alpine
    ports:
      - target: 80
        published: 80
        protocol: tcp
        mode: host
      - target: 443
        published: 443
        protocol: tcp
        mode: host
    networks:
      - frontend
    configs:
      - source: nginx_config
        target: /etc/nginx/nginx.conf
    deploy:
      mode: global # Deploy on all nodes
      placement:
        constraints:
          - node.role == manager
      update_config:
        parallelism: 1
        delay: 10s
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3

  # Noa Server (Main Application)
  noa-server:
    image: noa-server:latest
    networks:
      - frontend
      - backend
    environment:
      NODE_ENV: production
      DATABASE_URL: postgres://postgres@postgres:5432/noa
      REDIS_URL: redis://redis:6379
      RABBITMQ_URL: amqp://rabbitmq:5672
    secrets:
      - postgres_password
      - jwt_secret
      - api_key
    deploy:
      mode: replicated
      replicas: 5
      placement:
        max_replicas_per_node: 2
        constraints:
          - node.labels.type == app
      update_config:
        parallelism: 2
        delay: 10s
        failure_action: rollback
        monitor: 60s
      rollback_config:
        parallelism: 1
        delay: 5s
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M
      labels:
        - 'traefik.enable=true'
        - 'traefik.http.routers.noa-server.rule=Host(`noa-server.io`)'
        - 'traefik.http.services.noa-server.loadbalancer.server.port=3000'
    healthcheck:
      test: ['CMD', 'curl', '-f', 'http://localhost:3000/health']
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # MCP Server
  mcp-server:
    image: mcp-server:latest
    networks:
      - backend
    environment:
      NODE_ENV: production
      REDIS_URL: redis://redis:6379
    secrets:
      - api_key
    deploy:
      mode: replicated
      replicas: 3
      placement:
        constraints:
          - node.labels.type == app
      update_config:
        parallelism: 1
        delay: 10s
      restart_policy:
        condition: on-failure
      resources:
        limits:
          cpus: '1.5'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    healthcheck:
      test: ['CMD', 'curl', '-f', 'http://localhost:8080/health']
      interval: 30s
      timeout: 10s
      retries: 3

  # Background Workers
  worker:
    image: noa-worker:latest
    networks:
      - backend
    environment:
      NODE_ENV: production
      DATABASE_URL: postgres://postgres@postgres:5432/noa
      REDIS_URL: redis://redis:6379
      RABBITMQ_URL: amqp://rabbitmq:5672
    secrets:
      - postgres_password
    deploy:
      mode: replicated
      replicas: 3
      placement:
        constraints:
          - node.labels.type == worker
      update_config:
        parallelism: 1
        delay: 5s
      restart_policy:
        condition: on-failure
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M

  # PostgreSQL Database
  postgres:
    image: postgres:16-alpine
    networks:
      - database
    environment:
      POSTGRES_DB: noa
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD_FILE: /run/secrets/postgres_password
    secrets:
      - postgres_password
    volumes:
      - postgres_data:/var/lib/postgresql/data
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.labels.type == database
          - node.labels.storage == ssd
      restart_policy:
        condition: on-failure
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G
    healthcheck:
      test: ['CMD-SHELL', 'pg_isready -U postgres']
      interval: 10s
      timeout: 5s
      retries: 5

  # Redis Cache
  redis:
    image: redis:7-alpine
    networks:
      - backend
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --maxmemory 2gb --maxmemory-policy allkeys-lru
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.labels.type == cache
      restart_policy:
        condition: on-failure
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.25'
          memory: 512M
    healthcheck:
      test: ['CMD', 'redis-cli', 'ping']
      interval: 10s
      timeout: 5s
      retries: 5

  # RabbitMQ Message Queue
  rabbitmq:
    image: rabbitmq:3-management-alpine
    networks:
      - backend
    environment:
      RABBITMQ_DEFAULT_USER: admin
      RABBITMQ_DEFAULT_PASS: admin123
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.labels.type == queue
      restart_policy:
        condition: on-failure
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    healthcheck:
      test: ['CMD', 'rabbitmq-diagnostics', 'ping']
      interval: 30s
      timeout: 10s
      retries: 5

  # Prometheus Monitoring
  prometheus:
    image: prom/prometheus:latest
    networks:
      - backend
    volumes:
      - prometheus_data:/prometheus
    configs:
      - source: prometheus_config
        target: /etc/prometheus/prometheus.yml
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      restart_policy:
        condition: on-failure
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G

  # Grafana Dashboard
  grafana:
    image: grafana/grafana:latest
    networks:
      - backend
      - frontend
    volumes:
      - grafana_data:/var/lib/grafana
    environment:
      GF_SECURITY_ADMIN_PASSWORD: admin123
      GF_SERVER_ROOT_URL: https://grafana.noa-server.io
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      restart_policy:
        condition: on-failure
      labels:
        - 'traefik.enable=true'
        - 'traefik.http.routers.grafana.rule=Host(`grafana.noa-server.io`)'
        - 'traefik.http.services.grafana.loadbalancer.server.port=3000'

  # Node Exporter (Global service on all nodes)
  node-exporter:
    image: prom/node-exporter:latest
    networks:
      - backend
    command:
      - '--path.rootfs=/host'
    volumes:
      - /:/host:ro
    deploy:
      mode: global
      restart_policy:
        condition: on-failure
      resources:
        limits:
          cpus: '0.2'
          memory: 128M
        reservations:
          cpus: '0.1'
          memory: 64M

  # cAdvisor (Container metrics)
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    networks:
      - backend
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
    deploy:
      mode: global
      restart_policy:
        condition: on-failure
      resources:
        limits:
          cpus: '0.3'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 128M
